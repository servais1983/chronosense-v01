#!/usr/bin/env python3
"""
Chronosense v0.1 - Gestionnaire d'IA
G√®re la communication avec le mod√®le Phi-3 local et la g√©n√©ration d'hypoth√®ses

Auteur: G√©n√©r√© automatiquement
Version: 0.1 (Preuve de Concept)
"""

import json
import requests
from datetime import datetime
import time

# Import conditionnel pour Transformers (Hugging Face)
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è Transformers non disponible. Utilisation du mode simulation.")

class AIManager:
    """
    Gestionnaire d'IA pour l'analyse d'investigation DFIR
    Int√®gre le mod√®le Microsoft Phi-3 en local
    """
    
    def __init__(self):
        """
        Initialise le gestionnaire d'IA
        """
        self.model_name = "microsoft/Phi-3-mini-4k-instruct"
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        
        # Configuration
        self.max_tokens = 512
        self.temperature = 0.7
        self.api_url = "http://localhost:11434/api/generate"  # URL pour Ollama local
        
        # Mode de fonctionnement
        self.mode = "simulation"  # "transformers", "api", "simulation"
        
        # Initialiser le mod√®le
        self._initialize_model()
        
        print(f"AIManager initialis√© en mode: {self.mode}")
    
    def _initialize_model(self):
        """
        Initialise le mod√®le Phi-3 selon la m√©thode disponible
        """
        # Essayer d'abord avec Transformers (Hugging Face)
        if TRANSFORMERS_AVAILABLE:
            try:
                print("üîÑ Tentative de chargement du mod√®le Phi-3 via Transformers...")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    trust_remote_code=True,
                    torch_dtype="auto",
                    device_map="auto"
                )
                
                self.pipeline = pipeline(
                    "text-generation",
                    model=self.model,
                    tokenizer=self.tokenizer,
                    max_new_tokens=self.max_tokens,
                    temperature=self.temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
                
                self.mode = "transformers"
                print("‚úÖ Mod√®le Phi-3 charg√© avec succ√®s via Transformers")
                return
                
            except Exception as e:
                print(f"‚ùå Erreur lors du chargement via Transformers: {e}")
        
        # Essayer avec une API locale (Ollama, etc.)
        try:
            print("üîÑ Tentative de connexion √† l'API locale...")
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                self.mode = "api"
                print("‚úÖ API locale d√©tect√©e (Ollama)")
                return
        except Exception as e:
            print(f"‚ùå API locale non disponible: {e}")
        
        # Mode simulation par d√©faut
        self.mode = "simulation"
        print("‚ö†Ô∏è Mode simulation activ√© - Aucun mod√®le IA r√©el disponible")
    
    def generate_hypotheses(self, graph_description):
        """
        G√©n√®re des hypoth√®ses d'investigation bas√©es sur le graphe
        
        Args:
            graph_description (str): Description textuelle du graphe
            
        Returns:
            str: Hypoth√®ses g√©n√©r√©es par l'IA
        """
        # Construire le prompt structur√©
        prompt = self._build_investigation_prompt(graph_description)
        
        # G√©n√©rer la r√©ponse selon le mode disponible
        if self.mode == "transformers":
            return self._generate_with_transformers(prompt)
        elif self.mode == "api":
            return self._generate_with_api(prompt)
        else:
            return self._generate_simulation(graph_description)
    
    def _build_investigation_prompt(self, graph_description):
        """
        Construit le prompt structur√© pour l'analyse DFIR
        
        Args:
            graph_description (str): Description du graphe d'investigation
            
        Returns:
            str: Prompt format√© pour l'IA
        """
        prompt = f"""En tant qu'expert en analyse DFIR (Digital Forensics and Incident Response), analyse les art√©facts suivants issus d'une investigation en cours.

**Art√©facts connus :**
{graph_description}

**Ta mission :**
1. G√©n√®re 2 hypoth√®ses plausibles sur le type d'attaque en cours, bas√©es sur les TTPs du framework MITRE ATT&CK.
2. Pour chaque hypoth√®se, propose 1 action d'investigation concr√®te et imm√©diate que l'analyste devrait entreprendre pour la valider ou l'invalider.

Pr√©sente ta r√©ponse de mani√®re claire et concise.

**R√©ponse :**"""
        
        return prompt
    
    def _generate_with_transformers(self, prompt):
        """
        G√©n√®re une r√©ponse avec le mod√®le Transformers
        
        Args:
            prompt (str): Le prompt √† traiter
            
        Returns:
            str: R√©ponse g√©n√©r√©e
        """
        try:
            print("üß† G√©n√©ration avec Transformers...")
            
            # G√©n√©rer la r√©ponse
            outputs = self.pipeline(
                prompt,
                max_new_tokens=self.max_tokens,
                temperature=self.temperature,
                do_sample=True,
                return_full_text=False
            )
            
            response = outputs[0]['generated_text'].strip()
            
            # Post-traitement
            response = self._post_process_response(response)
            
            return response
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration: {e}")
            return f"Erreur lors de la g√©n√©ration d'hypoth√®ses: {e}"
    
    def _generate_with_api(self, prompt):
        """
        G√©n√®re une r√©ponse via API locale (Ollama)
        
        Args:
            prompt (str): Le prompt √† traiter
            
        Returns:
            str: R√©ponse g√©n√©r√©e
        """
        try:
            print("üß† G√©n√©ration via API locale...")
            
            payload = {
                "model": "phi3",  # Nom du mod√®le dans Ollama
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.temperature,
                    "num_predict": self.max_tokens
                }
            }
            
            response = requests.post(
                self.api_url,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get('response', '')
                return self._post_process_response(generated_text)
            else:
                return f"Erreur API: {response.status_code} - {response.text}"
                
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration via API: {e}")
            return f"Erreur lors de la g√©n√©ration d'hypoth√®ses: {e}"
    
    def _generate_simulation(self, graph_description):
        """
        G√©n√®re une r√©ponse simul√©e pour les tests
        
        Args:
            graph_description (str): Description du graphe
            
        Returns:
            str: R√©ponse simul√©e
        """
        print("üé≠ G√©n√©ration en mode simulation...")
        
        # Analyser les types d'art√©facts pr√©sents
        has_ip = 'ip:' in graph_description.lower() or any(x in graph_description for x in ['192.168', '10.', '172.', '8.8.8.8'])
        has_file = 'fichier:' in graph_description.lower() or '.exe' in graph_description.lower()
        has_process = 'processus:' in graph_description.lower() or 'powershell' in graph_description.lower()
        has_hash = 'hash:' in graph_description.lower() or len([x for x in graph_description.split() if len(x) >= 32 and all(c in '0123456789abcdefABCDEF' for c in x)]) > 0
        
        # G√©n√©rer des hypoth√®ses bas√©es sur les art√©facts d√©tect√©s
        hypotheses = []
        
        if has_ip and has_process:
            hypotheses.append({
                "title": "Hypoth√®se 1: Exfiltration de donn√©es via PowerShell",
                "description": "Les art√©facts sugg√®rent une possible exfiltration de donn√©es utilisant PowerShell pour communiquer avec des serveurs externes.",
                "mitre_ttp": "T1041 (Exfiltration Over C2 Channel), T1059.001 (PowerShell)",
                "action": "Analyser les logs r√©seau pour identifier les connexions sortantes et examiner l'historique des commandes PowerShell."
            })
        
        if has_file and has_hash:
            hypotheses.append({
                "title": "Hypoth√®se 2: D√©ploiement de malware",
                "description": "La pr√©sence de fichiers ex√©cutables avec des hash sp√©cifiques indique un possible d√©ploiement de malware.",
                "mitre_ttp": "T1204 (User Execution), T1105 (Ingress Tool Transfer)",
                "action": "V√©rifier les hash contre des bases de donn√©es de malware (VirusTotal, MISP) et analyser le comportement des fichiers."
            })
        
        if not hypotheses:
            # Hypoth√®ses g√©n√©riques
            hypotheses = [
                {
                    "title": "Hypoth√®se 1: Reconnaissance initiale",
                    "description": "Les art√©facts collect√©s sugg√®rent une phase de reconnaissance ou de d√©couverte du r√©seau.",
                    "mitre_ttp": "T1083 (File and Directory Discovery), T1057 (Process Discovery)",
                    "action": "Examiner les logs syst√®me pour identifier les activit√©s de d√©couverte et corr√©ler avec d'autres √©v√©nements suspects."
                },
                {
                    "title": "Hypoth√®se 2: Persistance syst√®me",
                    "description": "Les √©l√©ments d√©tect√©s pourraient indiquer une tentative d'√©tablissement de persistance sur le syst√®me.",
                    "mitre_ttp": "T1547 (Boot or Logon Autostart Execution), T1053 (Scheduled Task/Job)",
                    "action": "V√©rifier les m√©canismes de d√©marrage automatique et les t√¢ches planifi√©es pour d√©tecter des modifications suspectes."
                }
            ]
        
        # Formater la r√©ponse
        response_parts = []
        response_parts.append("ü§ñ **ANALYSE IA - HYPOTH√àSES D'INVESTIGATION**")
        response_parts.append("=" * 50)
        response_parts.append("")
        
        for i, hypothesis in enumerate(hypotheses, 1):
            response_parts.append(f"**{hypothesis['title']}**")
            response_parts.append("")
            response_parts.append(f"üìã **Description:** {hypothesis['description']}")
            response_parts.append("")
            response_parts.append(f"üéØ **TTPs MITRE ATT&CK:** {hypothesis['mitre_ttp']}")
            response_parts.append("")
            response_parts.append(f"üîç **Action recommand√©e:** {hypothesis['action']}")
            response_parts.append("")
            response_parts.append("-" * 40)
            response_parts.append("")
        
        response_parts.append("üí° **Note:** Cette analyse est g√©n√©r√©e en mode simulation.")
        response_parts.append("Pour une analyse compl√®te, configurez le mod√®le Phi-3 local.")
        response_parts.append("")
        response_parts.append(f"‚è∞ Analyse g√©n√©r√©e le {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')}")
        
        return "\n".join(response_parts)
    
    def _post_process_response(self, response):
        """
        Post-traite la r√©ponse de l'IA
        
        Args:
            response (str): R√©ponse brute de l'IA
            
        Returns:
            str: R√©ponse nettoy√©e et format√©e
        """
        # Nettoyer la r√©ponse
        response = response.strip()
        
        # Ajouter un timestamp
        timestamp = datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')
        response += f"\n\n‚è∞ Analyse g√©n√©r√©e le {timestamp}"
        
        return response
    
    def get_model_info(self):
        """
        Retourne les informations sur le mod√®le utilis√©
        
        Returns:
            dict: Informations sur le mod√®le
        """
        return {
            "model_name": self.model_name,
            "mode": self.mode,
            "available": self.mode != "simulation",
            "max_tokens": self.max_tokens,
            "temperature": self.temperature
        }
    
    def test_connection(self):
        """
        Teste la connexion au mod√®le d'IA
        
        Returns:
            bool: True si la connexion fonctionne
        """
        try:
            test_prompt = "Test de connexion. R√©ponds simplement 'OK'."
            
            if self.mode == "transformers":
                outputs = self.pipeline(test_prompt, max_new_tokens=10, do_sample=False)
                return "ok" in outputs[0]['generated_text'].lower()
            
            elif self.mode == "api":
                payload = {
                    "model": "phi3",
                    "prompt": test_prompt,
                    "stream": False,
                    "options": {"num_predict": 10}
                }
                response = requests.post(self.api_url, json=payload, timeout=30)
                return response.status_code == 200
            
            else:
                return True  # Mode simulation toujours disponible
                
        except Exception as e:
            print(f"‚ùå Test de connexion √©chou√©: {e}")
            return False


class PromptTemplates:
    """
    Templates de prompts pour diff√©rents types d'analyses DFIR
    """
    
    @staticmethod
    def investigation_analysis(artifacts):
        """
        Template pour l'analyse d'investigation g√©n√©rale
        """
        return f"""En tant qu'expert en analyse DFIR, analyse les art√©facts suivants:

{artifacts}

Fournis:
1. Deux hypoth√®ses d'attaque bas√©es sur MITRE ATT&CK
2. Des actions d'investigation concr√®tes pour chaque hypoth√®se
3. Une √©valuation du niveau de criticit√© (Faible/Moyen/√âlev√©/Critique)

R√©ponds de mani√®re structur√©e et professionnelle."""
    
    @staticmethod
    def malware_analysis(file_artifacts):
        """
        Template pour l'analyse de malware
        """
        return f"""Analyse les indicateurs de malware suivants:

{file_artifacts}

D√©termine:
1. Le type de malware probable
2. Les techniques d'√©vasion possibles
3. Les IOCs √† surveiller
4. Les mesures de containment recommand√©es"""
    
    @staticmethod
    def network_analysis(network_artifacts):
        """
        Template pour l'analyse r√©seau
        """
        return f"""Analyse les art√©facts r√©seau suivants:

{network_artifacts}

√âvalue:
1. Les patterns de communication suspects
2. Les indicateurs de C2 (Command & Control)
3. Les signes d'exfiltration de donn√©es
4. Les recommandations de monitoring r√©seau"""
